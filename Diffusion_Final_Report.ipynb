{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e05e711-7547-477a-b1fe-64eeecd90cdd",
   "metadata": {},
   "source": [
    "<h1><center> Goals and Discussion </center> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408aee31-8210-4f1f-8702-a159e20d0c57",
   "metadata": {},
   "source": [
    "We implemented a diffusion model to generate images trained on the CIFAR-10 dataset. Diffusion models are a class of generative models which have achieved state of the art performance on standard image generation tests, and underly many recent advancements in generative modeling. In the context of image generation, diffusion models work by systematically adding noise to the training data through a Gaussian process. A neural network is then trained to predict the noise added to the image at each timestep, ultimately allowing images to be generated from a random sample of a standard Gaussian.\n",
    "\n",
    "The CIFAR-10 dataset consists of 60,000 32x32 pixel RBG images across 10 classes, and we used various subsets of this dataset to train our model. We trained an unconditional model on a single class (cats) from the CIFAR-10 dataset, as well as a conditional model on multiple classes. See below for some sample generated images from our model.\n",
    "\n",
    "In what follows, we will recount our goals from our revised proposal and discuss the progress we made towards these goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c11818-be47-451a-828e-cb476df25f73",
   "metadata": {},
   "source": [
    "<h2> Essential Goals </h2>\n",
    "\n",
    "Each bullet point below is one of the essential goals we submitted in our\n",
    "\n",
    "- **Essential Goal 1:** Our plan is to create the model from scratch following the implementation from Ho et al. We will be using the implementation from https://www.youtube.com/watch?v=a4Yfz2FxXiY as reference if we get stuck. One change we may make right away is to use a simpler architecture for the neural network, since U-Nets are expensive to train.\n",
    "\n",
    "We successfully implemented the diffusion model from scratch, but ultimately appealed to a pre-built U-NET neural network as our denoising net. In particular, our code implements both algorithms 1 (training) and 2 (sampling) from Ho et al. The training algorithm presented in the paper is a simplification of a more general loss function (see equation (5) in loc. cit.) which we also implemented in code, although all of the experiments we will discuss in this report use the simplified loss function of the paper.\n",
    "\n",
    "We organized our code in to three classes: the diffusion_process, the denoiser, and the diff_model.\n",
    "\n",
    "The diffusion_process class takes as input the number of timesteps as well as the initial and final steps of the variance schedule. Using the inputs, the total variance schedule is generated by linearly interpolating between the initial and final variances using the specified number of timesteps. The diffusion process then contains two functions. Denoting by $q(x_{t}|x_{t-1})$ the Gaussian distributions defining the Gaussian process (determined by the variance schedule) the diffusion_process class contains a function 'forward_var' which computes the variance of the distribution $q(x_{t}|x_{0})$ from the variance schedule. The second function takes in an image and a number of timesteps, and adds noise to the image for the specified timesteps.\n",
    "\n",
    "The denoiser class takes as input a neural network and uses the neural network to remove noise from images. In retrospect, this class should have been absorbed in the diff_model class which we discuss next.\n",
    "\n",
    "The diff_model class organizes the utilities needed to compute the loss function (equation (5) in Ho et. al.), generate images (algorithm 2 in loc. cit.), and generate conditional samples (a simple variant of algorithm 2 in loc. cit.).\n",
    "\n",
    "\n",
    "The U-NET neural network was taken from the github repository https://github.com/dome272/Diffusion-Models-pytorch, but we explored many variants of this architecture. These variants will be discussed in the 'desirable goals' section.\n",
    "\n",
    "\n",
    "See the sample images below.\n",
    "\n",
    "\n",
    "\n",
    "- **Essential Goal 2:** We will evaluate the performance of the model using this FID implementation: https://pytorch.org/ignite/generated/ignite.metrics.FID.html. \n",
    "\n",
    "- **Essential Goal 3:** We are revising this goal to be the training of a DCGAN model on CIFAR-10 following https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html. We will keep both this and the diffusion model unconditional at first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53ac791-a591-4d0c-a6dc-23e33372534b",
   "metadata": {},
   "source": [
    "<h2><center> Selected Generated Images </center> </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e04e680-40ce-4078-af2f-6464819326f9",
   "metadata": {},
   "source": [
    "![image](notebooks/Diffusion/images/Conditional_T=500_frogs.png)\n",
    "\n",
    "![image](notebooks/Diffusion/images/Conditional_cats.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14d5db0-15fe-469f-862c-e498c5af1e7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2> Desirable Goals </h2>\n",
    "\n",
    "- We will compare different neural network architectures such as U-Net and simpler architectures (e.g. a CNN).\n",
    "\n",
    "We experimented with six distinct U-Net structures of varying sizes. The U-Net architecture involves a series of downsampling layers, a bottleneck, and a series of upsampling layers. The downsampling layers consist of a stack of a convolutional layer, a self-attention layer, and a time-embedding to keep track of the time-steps in the diffusion process. The downsampling shrinks the image size, and increases the number of channels. The upsampling layers are similar stacks of three distinct layer types, but shrinks the number of channels to the usual three, outputting an image at the end. The bottleneck consists of a series of convolutional layers.\n",
    "\n",
    "- We will experiment with hyperparameters such as noise schedule, number of timesteps, and width/size of NN layers to improve performance.\n",
    "\n",
    "We performed experiments probing the effects of all of these hyperparameters.\n",
    "\n",
    "The largest model we experimented with consisted of three downsampling layers, three upsampling layers, and a bottleneck with three layers. The smallest model we worked with shrunk the number of downsampling/upsampling/bottleneck layers to two as well as decreasing the number of channels at each layer. The smallest models trained much faster (approximately 1000 epochs in 2 hours with a GPU) whereas the largest was much slower (approximately 1000 epochs in 7 hours with a GPU). \n",
    "\n",
    "Using the largest model, we compared the effect of timesteps by training a model with 500, 1000, and 1500 timesteps respectively, all for 500 epochs. Larger numbers of timesteps resulted in predictably slower train times, but surprisingly didn't appear to generate perceptibly higher quality images (see below for samples).\n",
    "\n",
    "- Our diffusion model will do conditional generation, following the paper example and reference implementation.\n",
    "\n",
    "We successfully implemented class conditioning in our model. This involves incorporating a class embedding inside the U-Net structure, akin to the timestep embedding. Interestingly, training a conditional model on two classes (cats and frogs) produced better images of cats than training an unconditional model only on cats.\n",
    "\n",
    "- We will compare our GAN against our diffusion model using FID score. The architecture of the GAN will follow the above pytorch tutorial, but we will add in class conditioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079e644d-88c7-4839-82af-1326480a8367",
   "metadata": {},
   "source": [
    "<h2><center> Samples from a Conditional Model </center> </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2517da4c-7c4a-4237-bc22-534f3d69200c",
   "metadata": {},
   "source": [
    "The following samples were generated by the same conditional model.\n",
    "\n",
    "![image](/output/conditional_images/Conditional_T=1000_frogs.png)\n",
    "![image](notebooks/Diffusion/images/Conditional_cats.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eee14e2-64e1-458d-a946-dcca5b0bdfae",
   "metadata": {},
   "source": [
    "<h2><center> Frog Samples from Timestep Experiments </center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d534cec8-0100-47ac-98a2-ac078f6382e8",
   "metadata": {},
   "source": [
    "$T = 500$\n",
    "\n",
    "![image](notebooks/Diffusion/images/Conditional_T=500_frogs.png)\n",
    "\n",
    "\n",
    "$T = 1000$\n",
    "\n",
    "![image](notebooks/Diffusion/images/Conditional_frogs.png)\n",
    "\n",
    "$T = 1500$\n",
    "\n",
    "![image](notebooks/Diffusion/images/Conditional_T=1500_frogs.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9342da15-e7de-4d9a-9514-5a0b6fa33f96",
   "metadata": {},
   "source": [
    "<h1><center> Code Documentation </center></h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
