{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1599d0ed6f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "import uuid\n",
    "\n",
    "from modules import UNet\n",
    "from modules import UNet_2xUD\n",
    "from modules import UNet_2xUD_halfchannels\n",
    "\n",
    "from\n",
    "\n",
    "seed = 449\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "args = types.SimpleNamespace()\n",
    "args.run_name = \"1024ep-16batch-3e4lr-UNet_2xUD_halfchannels\"\n",
    "args.epochs = 1024\n",
    "args.batch_size = 16\n",
    "args.image_size = 32\n",
    "args.device = \"cuda\"\n",
    "args.lr = 3e-4\n",
    "args.model = \"UNet_2xUD_halfchannels\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Added code needed from CatDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(images, path, **kwargs):\n",
    "    # grid = torchvision.utils.make_grid(images, **kwargs)\n",
    "    ndarr = images.permute(1, 2, 0).to('cpu').numpy()\n",
    "    im = Image.fromarray(ndarr)\n",
    "    im.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion:\n",
    "    # diffusion tools: noise schedule, forward process (noising), sampling images\n",
    "    def __init__(self, noise_steps=1000, beta_0=1e-4, beta_T=0.02, img_size=32, device=\"cuda\"):\n",
    "        self.noise_steps = noise_steps\n",
    "        self.beta_0 = beta_0\n",
    "        self.beta_T = beta_T\n",
    "        self.img_size = img_size\n",
    "        self.device = device\n",
    "\n",
    "        self.beta = self.noise_schedule(\"linear\").to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "        self.flag = 0\n",
    "        self.fixed = None\n",
    "    \n",
    "    def noise_schedule(self, type):\n",
    "        \"\"\"\n",
    "        Creates the noise schedule used by the diffusion class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        type : {\"linear\", }\n",
    "            The type of schedule to be created.\n",
    "        \"\"\"\n",
    "\n",
    "        if type == \"linear\":\n",
    "            return torch.linspace(self.beta_0, self.beta_T, self.noise_steps)\n",
    "    \n",
    "    def noise_images(self, x, t):\n",
    "        \"\"\"\n",
    "        Noises image using the reparameterization trick to noise in a single step.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Tensor\n",
    "            Image noised to timestep t\n",
    "        epsilon : Tensor\n",
    "            Noise added to image\n",
    "        \"\"\"\n",
    "\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None] #TODO why this dimension?\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1. - self.alpha_hat[t])[:, None, None, None]\n",
    "        epsilon = torch.randn_like(x)\n",
    "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * epsilon, epsilon\n",
    "\n",
    "    def sample_timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
    "    \n",
    "    def sample(self, model, n, disable_bar=True, out=None, fixed=False):\n",
    "        # n: number of images to sample\n",
    "        # algorithm 2 from DDPM paper\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # generate random vector as starting point, or use fixed noise\n",
    "            if (fixed == False):\n",
    "                x = torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)\n",
    "            else:\n",
    "                if self.flag == 0:\n",
    "                    self.fixed = torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)\n",
    "                    self.flag = 1\n",
    "                x = self.fixed\n",
    "            # run through all steps to denoise image\n",
    "            for i in tqdm(reversed(range(1, self.noise_steps)), disable=disable_bar):\n",
    "                t = (torch.ones(n) * i).long().to(self.device)\n",
    "                predicted_noise = model(x, t)\n",
    "                alpha = self.alpha[t][:, None, None, None]\n",
    "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
    "                beta = self.beta[t][:, None, None, None]\n",
    "\n",
    "                if i > 1:\n",
    "                    # TODO: why are we adding noise for backwards process?\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    # don't add noise for the last step\n",
    "                    noise = torch.zeros_like(x)\n",
    "\n",
    "                # denoise image using algorithm 2 step 4\n",
    "                x = 1 / torch.sqrt(alpha) * (x - ((1-alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "\n",
    "                # save in array if given\n",
    "                if out is not None:\n",
    "                    generated = (x.clamp(-1, 1) + 1) / 2\n",
    "                    generated = (generated*255).type(torch.uint8)\n",
    "                    generated = torchvision.utils.make_grid(generated).cpu().numpy()\n",
    "                    out.append(generated)\n",
    "        model.train()\n",
    "        x = (x.clamp(-1, 1) + 1) / 2\n",
    "        x = (x*255).type(torch.uint8)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m\n\u001b[0;32m     13\u001b[0m         model \u001b[39m=\u001b[39m UNet_2xUD_halfchannels()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     16\u001b[0m \u001b[39m# structure reduced to 2x up down blocks\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m# args.run_name = \"1e4lr\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m# path = os.path.join(\"models/CatDiffusion/\", args.run_name, \"ckpt.pt\")\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m# model = UNet_2xUD().to(\"cuda\")\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(path))\n\u001b[0;32m     22\u001b[0m diffusion \u001b[39m=\u001b[39m Diffusion(img_size\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mimage_size, device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m iarr \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    808\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 809\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m    810\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[0;32m    811\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1170\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1171\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1172\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1174\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1141\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1142\u001b[0m     typed_storage \u001b[39m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[0;32m   1144\u001b[0m \u001b[39mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:1116\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1112\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39mUntypedStorage)\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_untyped_storage\n\u001b[0;32m   1113\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1114\u001b[0m \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[1;32m-> 1116\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[0;32m   1117\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m   1118\u001b[0m     _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m typed_storage\u001b[39m.\u001b[39m_data_ptr() \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1121\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m typed_storage\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 217\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[0;32m    218\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:182\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[0;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 182\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[0;32m    183\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    184\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:166\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m    163\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    168\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    169\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    170\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    171\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "# original structure\n",
    "\n",
    "\n",
    "path = os.path.join(\"models/CatDiffusion/\", args.run_name, \"ckpt.pt\")\n",
    "if (args.model == \"UNet\"):\n",
    "    model = UNet().to(\"cpu\")\n",
    "elif (args.model == \"UNet_2xUD\"):\n",
    "    model = UNet_2xUD().to(\"cuda\")\n",
    "elif (args.model == \"UNet_2xUD_halfchannels\"):\n",
    "        model = UNet_2xUD_halfchannels().to(\"cuda\")\n",
    "\n",
    "\n",
    "# structure reduced to 2x up down blocks\n",
    "# args.run_name = \"1e4lr\"\n",
    "# path = os.path.join(\"models/CatDiffusion/\", args.run_name, \"ckpt.pt\")\n",
    "# model = UNet_2xUD().to(\"cuda\")\n",
    "\n",
    "model.load_state_dict(torch.load(path))\n",
    "diffusion = Diffusion(img_size=args.image_size, device=\"cuda\")\n",
    "\n",
    "iarr = []\n",
    "sampled_images = diffusion.sample(model, n=32, disable_bar=False, out=iarr)\n",
    "img_grid = torchvision.utils.make_grid(sampled_images)\n",
    "save_images(img_grid, os.path.join(\"output/\", args.run_name, f\"{uuid.uuid4()}.png\"))\n",
    "\n",
    "ndarr = img_grid.permute(1, 2, 0).cpu().numpy()\n",
    "plt.figure()\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(ndarr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
